{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCS Inventory Loader Introduction\n",
    "Load your GCS bucket inventory into BigQuery (or stdout) fast with this tool.\n",
    "\n",
    "It can be very useful to have an inventory of your GCS objects and their metadata, particularly in a powerful database like BigQuery. The GCS listing API supports filtering by prefixes, but more complex queries can't be done via API. Using a database, you can find out lots of information about the data you have in GCS, such as finding very large objects, very old or stale objects, etc.\n",
    "\n",
    "This utility will help you bulk load an object listing to stdout, or directly into BigQuery. It can also help you keep your inventory up-to-date with the listen command.\n",
    "\n",
    "The implementation here takes the approach of listing buckets and sending each page to a worker in a thread pool for processing and streaming into BigQuery. Throughput rates of 15s per 100,000 objects have been achieved with moderately sized (32 vCPU) virtual machines. This works out to 2 minutes and 30 seconds per million objects. Note that this throughput is per process -- simply shard the bucket namespace across multiple projects to increase this throughput.\n",
    "\n",
    "## Costs\n",
    "Compute costs notwithstanding, the primary cost you'll incur for listing objects is Class A operations charges. Under most circumstances you'll get a listing with 1000 objects per page (exceptional circumstances might be... you just did a lot of deletes and the table is sparse). So cost is figured like so:\n",
    "\n",
    "`(number of objects listed) / 1000 / 10,000 * (rate per 10,000 class A ops)`\n",
    "\n",
    "For example, in a standard regional bucket, listing 100 million objects should cost about .5 USD:\n",
    "\n",
    "`(100 million) / 1000 / 10,000 * $0.05 = $0.50`\n",
    "\n",
    "\n",
    "## To Learn More\n",
    "To learn more go to https://github.com/domZippilli/gcs-inventory-loader\n",
    "\n",
    "# Set up the Notebook\n",
    "\n",
    "For this notebook to run you need to have a BigQuery project and dataset already created, so the code knows where to load the data.\n",
    "\n",
    "## Clone gcs-inventory-loader Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/domZippilli/gcs-inventory-loader.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gcs-inventory-loader/.\n",
    "!cd gcs-inventory-loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the default.cfg file\n",
    "We need to configure the .default.cfg file to point to the right BQ project for the resulting dataset and also the project within which the buckets of interest live\n",
    "\n",
    "Change These Values Below:\n",
    "- PROJECT=The_BigQuery_Project\n",
    "- GCS_PROJECT=The_Bucket_Project\n",
    "- DATASET_NAME=The_BigQuery_Dataset\n",
    "- INVENTORY_TABLE=The_BigQuery_Table\n",
    "\n",
    "Helpful Tip: You don't need quotes around the value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The project use for BigQuery and where the dataset and table of the uploaded object metadata can be accessed\n",
    "PROJECT=\"CONFIGURE_ME\"\n",
    "\n",
    "# The project use for google storge where the notebook bucket live\n",
    "GCS_PROJECT=\"CONFIGURE_ME\"\n",
    "\n",
    "# The resulting dataset of gcs-inventory-loader where the table of the uploaded object metadata will be store \n",
    "DATASET_NAME=\"CONFIGURE_ME\"\n",
    "\n",
    "# The resulting table of gcs-inventory-loader where the object metadata will be uploaded\n",
    "INVENTORY_TABLE=\"CONFIGURE_ME\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cell magic to edit the file\n",
    "from IPython.core.magic import register_line_cell_magic\n",
    "\n",
    "@register_line_cell_magic\n",
    "def writetemplate(line, cell):\n",
    "    with open(line, 'w') as f:\n",
    "        f.write(cell.format(**globals()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writetemplate default.cfg\n",
    "# Copyright 2019 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\n",
    "[GCP]\n",
    "# The project in which to scan for buckets and load object information into a table.\n",
    "PROJECT={PROJECT}\n",
    "\n",
    "# The project in which to scan for buckets / objects only. Use this (or BIGQUERY.JOB_PROJECT) if to span BQ and GCS across different projects.\n",
    "GCS_PROJECT={GCS_PROJECT}\n",
    "\n",
    "[RUNTIME]\n",
    "# Number of worker threads. Two threads will be reserved for listing buckets, and the remaining threads will be used to send list pages into BigQuery. Even on a single core machine, this should be set to at least 4 to allow for context switches during IO waits.\n",
    "WORKERS=64\n",
    "\n",
    "# Amount of work items (page listings) to store. More items will use more memory, but a larger work queue can improve performance if you see throughput stuttering.\n",
    "WORK_QUEUE_SIZE=1000\n",
    "\n",
    "# Log level for the inventory loader. Default is INFO.\n",
    "# LOG_LEVEL=DEBUG\n",
    "\n",
    "\n",
    "[BIGQUERY]\n",
    "# The dataset to use for inventory data.\n",
    "DATASET_NAME={DATASET_NAME}\n",
    "\n",
    "# A table in which to place the object inventory.\n",
    "INVENTORY_TABLE={INVENTORY_TABLE}\n",
    "\n",
    "# How many rows to stream into BigQuery before starting a new stream.\n",
    "# Default is 100, which is conservative, but most configurations can run much larger. Higher numbers use more memory, and an excessively high number may hit BQ limits.\n",
    "BATCH_WRITE_SIZE=500\n",
    "\n",
    "# Project to use for running BQ jobs. This is useful if you want to run the job in one project but store the data in another.\n",
    "# Default is GCP.PROJECT\n",
    "# JOB_PROJECT=\n",
    "\n",
    "#[PUBSUB]\n",
    "# The topic to listen to for object updates. Just give the short name of the topic, not the fully qualified name.\n",
    "#TOPIC_SHORT_NAME=gcs_updates\n",
    "\n",
    "# The subscription to listen to for object updates (will be created if not found)\n",
    "#SUBSCRIPTION_SHORT_NAME=gcs_updates_sub_01\n",
    "\n",
    "# The message wait timeout in seconds. Defaults to 10.\n",
    "# This value shouldn't need adjusting. During the 10 second wait, notifications that are enqueued to be written to BigQuery\n",
    "# could be lost in the event of a KP/plug-pull. If you need to shorten this window, you probably should also shrink the batch write size.\n",
    "# TIMEOUT="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load your workspace bucket to a BQ project\n",
    "If you've configured the config file correctly, you should be able to get your bucket inventory loaded with a simple command. Note that by default, this will load an inventory of all objects for all buckets in your project. \n",
    "\n",
    "- Make sure you ADD your Proxy email (found in Terra Profile) to the BigQuery project as an Editor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Getting Workspace Bucket\n",
    "bucket = os.environ[\"WORKSPACE_BUCKET\"].split(\"/\")[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcs_inventory load $bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize object metadata\n",
    "\n",
    "Now the data is in BigQuery, we can run analytics on the BigQuery table to help manage your storage data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This sets the path parameter for BigQuery\n",
    "params= {\"path\": f\"{PROJECT}.{DATASET_NAME}.{INVENTORY_TABLE}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Size \n",
    "\n",
    "The Bar Graph will show the GB file Size per all the same files types, so you can drive deeper in how many large and small files you have collectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext google.cloud.bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery data_size_table --use_rest_api --params $params\n",
    "DECLARE path STRING;\n",
    "SET path = @path;\n",
    "EXECUTE IMMEDIATE CONCAT('SELECT REGEXP_EXTRACT(name, r\"\\\\.[0-9a-z]+$\") AS file_extension, ROUND(SUM(size) / 1000 / 1000 / 1000, 1) AS sizeGB, COUNT(*) AS files, FROM `',path,'` GROUP BY file_extension ORDER BY sizeGB DESC');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size_table.plot(kind=\"bar\", x=\"file_extension\", y=\"sizeGB\", figsize=(50,20), fontsize=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files per File Type\n",
    "\n",
    "The Pie Graph will show how many file of each file type is in your bucket \n",
    "\n",
    "This will show an general overview Pie Graph and an detailed Pie Graph\n",
    "\n",
    "### General Overview Pie Graph for Files per File Type\n",
    "\n",
    "Includes standard file types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Pie chart, where the slices will be ordered and plotted counter-clockwise:\n",
    "types = list(data_size_table[\"file_extension\"])\n",
    "\n",
    "# Check if there is files\n",
    "if len(types) > 0:\n",
    "    sizes = np.array(list(data_size_table[\"sizeGB\"]))\n",
    "    porcent = 100.*sizes/sizes.sum()\n",
    "\n",
    "    labels = ['{0} - {1:1.2f} %'.format(i,j) for i,j in zip(types, porcent)]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    patches, text = plt.pie(sizes, radius=5, startangle=90)\n",
    "\n",
    "\n",
    "    plt.legend(patches, labels, loc=\"right\", bbox_to_anchor=(-0.1, 1.),\n",
    "               fontsize=10)\n",
    "\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"You Have No Files - Plots Won't Work\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Detailed  Pie Graph for Files per File Type\n",
    "\n",
    "Includes logs, stderr, stdout, meta and more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you didn't run it before\n",
    "%load_ext google.cloud.bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery file_type_data --use_rest_api --params $params\n",
    "DECLARE path STRING;\n",
    "SET path = @path;\n",
    "EXECUTE IMMEDIATE CONCAT('SELECT IF( LENGTH(ifnull(ARRAY_REVERSE(SPLIT(ARRAY_REVERSE(SPLIT(name, \"/\"))[OFFSET(0)], \".\"))[SAFE_OFFSET(0)], ARRAY_REVERSE(SPLIT(name, \"/\"))[OFFSET(0)])) > 16 , ARRAY_REVERSE(SPLIT(name, \"/\"))[OFFSET(1)], ifnull(ARRAY_REVERSE(SPLIT(ARRAY_REVERSE(SPLIT(name, \"/\"))[OFFSET(0)], \".\"))[SAFE_OFFSET(0)], ARRAY_REVERSE(SPLIT(name, \"/\"))[OFFSET(0)])) as type, COUNT(name) as count FROM `',path,'` GROUP BY type HAVING COUNT(name) >1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_type_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Pie chart, where the slices will be ordered and plotted counter-clockwise:\n",
    "types = list(file_type_data[\"type\"])\n",
    "\n",
    "# Check if there is files\n",
    "if len(types) > 0:\n",
    "    sizes = np.array(list(file_type_data[\"count\"]))\n",
    "    porcent = 100.*sizes/sizes.sum()\n",
    "\n",
    "    labels = ['{0} - {1:1.2f} %'.format(i,j) for i,j in zip(types, porcent)]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    patches, text = plt.pie(sizes, radius=5, startangle=90)\n",
    "\n",
    "\n",
    "    plt.legend(patches, labels, loc=\"right\", bbox_to_anchor=(-0.1, 1.),\n",
    "               fontsize=10)\n",
    "\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"You Have No Files - Plots Won't Work\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicate Files\n",
    "\n",
    "Using the file path and Hash number combined, the queries finds all the duplicate files and returns a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you didn't run it before\n",
    "%load_ext google.cloud.bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery file_Duplicate --use_rest_api --params $params\n",
    "DECLARE path STRING;\n",
    "SET path = @path;\n",
    "EXECUTE IMMEDIATE CONCAT('SELECT Concat(regexp_replace(regexp_replace(name, cast(ARRAY_REVERSE(SPLIT(name, r\"/\"))[SAFE_OFFSET(0)] as string), r\"\"), cast(ARRAY_REVERSE(SPLIT(name, r\"/\"))[SAFE_OFFSET(1)] as string), r\"\"), r\"-\", cast( md5Hash as string)) as file_name, Count(regexp_replace(regexp_replace(name, cast(ARRAY_REVERSE(SPLIT(name, r\"/\"))[OFFSET(0)] as string), r\"\"), cast(ARRAY_REVERSE(SPLIT(name, r\"/\"))[SAFE_OFFSET(1)] as string), r\"\")) as count FROM `',path,'` Group by file_name Having Count(file_name) > 2 ORDER By COUNT DESC')                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None, 'display.max_colwidth', None)\n",
    "file_Duplicate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Timeline\n",
    "\n",
    "This Bar graph shows a timeline of when files were created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you didn't run it before\n",
    "%load_ext google.cloud.bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery data_timeline_table --use_rest_api --params $params\n",
    "DECLARE path STRING;\n",
    "SET path = @path;\n",
    "EXECUTE IMMEDIATE CONCAT('SELECT DATE(timeCreated) as date,  count(name) as name_count FROM `',path,'` Group By DATE(timeCreated)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_timeline_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_timeline_table.plot(kind=\"bar\", x=\"date\", y=\"name_count\", figsize=(50,20), fontsize=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
