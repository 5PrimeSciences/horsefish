{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCS Inventory Loader Introduction\n",
    "Load your GCS bucket inventory into BigQuery (or stdout) fast with this tool.\n",
    "\n",
    "It can be very useful to have an inventory of your GCS objects and their metadata, particularly in a powerful database like BigQuery. The GCS listing API supports filtering by prefixes, but more complex queries can't be done via API. Using a database, you can find out lots of information about the data you have in GCS, such as finding very large objects, very old or stale objects, etc.\n",
    "\n",
    "This utility will help you bulk load an object listing to stdout, or directly into BigQuery. It can also help you keep your inventory up-to-date with the listen command.\n",
    "\n",
    "The implementation here takes the approach of listing buckets and sending each page to a worker in a thread pool for processing and streaming into BigQuery. Throughput rates of 15s per 100,000 objects have been achieved with moderately sized (32 vCPU) virtual machines. This works out to 2 minutes and 30 seconds per million objects. Note that this throughput is per process -- simply shard the bucket namespace across multiple projects to increase this throughput.\n",
    "\n",
    "## Costs\n",
    "Compute costs notwithstanding, the primary cost you'll incur for listing objects is Class A operations charges. Under most circumstances you'll get a listing with 1000 objects per page (exceptional circumstances might be... you just did a lot of deletes and the table is sparse). So cost is figured like so:\n",
    "\n",
    "`(number of objects listed) / 1000 / 10,000 * (rate per 10,000 class A ops)`\n",
    "\n",
    "For example, in a standard regional bucket, listing 100 million objects should cost about .5 USD:\n",
    "\n",
    "`(100 million) / 1000 / 10,000 * $0.05 = $0.50`\n",
    "\n",
    "\n",
    "## To Learn More\n",
    "To learn more go to https://github.com/domZippilli/gcs-inventory-loader\n",
    "\n",
    "# Clone gcs-inventory-loader Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/domZippilli/gcs-inventory-loader.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd gcs-inventory-loader\n",
    "!pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure the default.cfg file\n",
    "Configure the .default.cfg file to point to the right BQ project for the resulting dataset and also the project within which the buckets of interest live\n",
    "\n",
    "Change These Values:\n",
    "- PROJECT=The_BigQuery_Project\n",
    "- GCS_PROJECT=The_Bucket_Project\n",
    "- DATASET_NAME=The_BigQuery_Dataset\n",
    "- INVENTORY_TABLE=The_BigQuery_Table\n",
    "\n",
    "Helpful Tip: You don't need quotes around the value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile default.cfg\n",
    "# Copyright 2019 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\n",
    "[GCP]\n",
    "# The project in which to scan for buckets and load object information into a table.\n",
    "PROJECT=CONFIGURE_ME\n",
    "\n",
    "# The project in which to scan for buckets / objects only. Use this (or BIGQUERY.JOB_PROJECT) if to span BQ and GCS across different projects.\n",
    "GCS_PROJECT=CONFIGURE_ME\n",
    "\n",
    "[RUNTIME]\n",
    "# Number of worker threads. Two threads will be reserved for listing buckets, and the remaining threads will be used to send list pages into BigQuery. Even on a single core machine, this should be set to at least 4 to allow for context switches during IO waits.\n",
    "WORKERS=64\n",
    "\n",
    "# Amount of work items (page listings) to store. More items will use more memory, but a larger work queue can improve performance if you see throughput stuttering.\n",
    "WORK_QUEUE_SIZE=1000\n",
    "\n",
    "# Log level for the inventory loader. Default is INFO.\n",
    "# LOG_LEVEL=DEBUG\n",
    "\n",
    "\n",
    "[BIGQUERY]\n",
    "# The dataset to use for inventory data.\n",
    "DATASET_NAME=CONFIGURE_ME\n",
    "\n",
    "# A table in which to place the object inventory.\n",
    "INVENTORY_TABLE=object_metadata\n",
    "\n",
    "# How many rows to stream into BigQuery before starting a new stream.\n",
    "# Default is 100, which is conservative, but most configurations can run much larger. Higher numbers use more memory, and an excessively high number may hit BQ limits.\n",
    "BATCH_WRITE_SIZE=500\n",
    "\n",
    "# Project to use for running BQ jobs. This is useful if you want to run the job in one project but store the data in another.\n",
    "# Default is GCP.PROJECT\n",
    "# JOB_PROJECT=\n",
    "\n",
    "#[PUBSUB]\n",
    "# The topic to listen to for object updates. Just give the short name of the topic, not the fully qualified name.\n",
    "#TOPIC_SHORT_NAME=gcs_updates\n",
    "\n",
    "# The subscription to listen to for object updates (will be created if not found)\n",
    "#SUBSCRIPTION_SHORT_NAME=gcs_updates_sub_01\n",
    "\n",
    "# The message wait timeout in seconds. Defaults to 10.\n",
    "# This value shouldn't need adjusting. During the 10 second wait, notifications that are enqueued to be written to BigQuery\n",
    "# could be lost in the event of a KP/plug-pull. If you need to shorten this window, you probably should also shrink the batch write size.\n",
    "# TIMEOUT="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Bucket Contents into BigQuery\n",
    "If you've configured the config file correctly, you should be able to get your bucket inventory loaded with a simple command. Note that by default, this will load an inventory of all objects for all buckets in your project. \n",
    "\n",
    "- Make sure you ADD your Proxy email (found in Terra Profile) to the BigQuery project as an Editor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcs_inventory load <BUCKET: fc-****-****-****-****>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
